Total: 52.38s
ROUTINE ======================== internal/sync.(*Mutex).unlockSlow in /opt/homebrew/Cellar/go/1.24.3/libexec/src/internal/sync/mutex.go
      30ms     15.37s (flat, cum) 29.34% of Total
      10ms       10ms    202:func (m *Mutex) unlockSlow(new int32) {
         .          .    203:	if (new+mutexLocked)&mutexLocked == 0 {
         .          .    204:		fatal("sync: unlock of unlocked mutex")
         .          .    205:	}
         .          .    206:	if new&mutexStarving == 0 {
         .          .    207:		old := new
         .          .    208:		for {
         .          .    209:			// If there are no waiters or a goroutine has already
         .          .    210:			// been woken or grabbed the lock, no need to wake anyone.
         .          .    211:			// In starvation mode ownership is directly handed off from unlocking
         .          .    212:			// goroutine to the next waiter. We are not part of this chain,
         .          .    213:			// since we did not observe mutexStarving when we unlocked the mutex above.
         .          .    214:			// So get off the way.
         .          .    215:			if old>>mutexWaiterShift == 0 || old&(mutexLocked|mutexWoken|mutexStarving) != 0 {
         .          .    216:				return
         .          .    217:			}
         .          .    218:			// Grab the right to wake someone.
         .          .    219:			new = (old - 1<<mutexWaiterShift) | mutexWoken
      10ms       10ms    220:			if atomic.CompareAndSwapInt32(&m.state, old, new) {
         .     15.34s    221:				runtime_Semrelease(&m.sema, false, 2)
      10ms       10ms    222:				return
         .          .    223:			}
         .          .    224:			old = m.state
         .          .    225:		}
         .          .    226:	} else {
         .          .    227:		// Starving mode: handoff mutex ownership to the next waiter, and yield
ROUTINE ======================== runtime.goparkunlock in /opt/homebrew/Cellar/go/1.24.3/libexec/src/runtime/proc.go
         0       10ms (flat, cum) 0.019% of Total
         .          .    440:func goparkunlock(lock *mutex, reason waitReason, traceReason traceBlockReason, traceskip int) {
         .       10ms    441:	gopark(parkunlock_c, unsafe.Pointer(lock), reason, traceReason, traceskip)
         .          .    442:}
         .          .    443:
         .          .    444:// goready should be an internal detail,
         .          .    445:// but widely used packages access it using linkname.
         .          .    446:// Notable members of the hall of shame include:
ROUTINE ======================== runtime.parkunlock_c in /opt/homebrew/Cellar/go/1.24.3/libexec/src/runtime/proc.go
         0      480ms (flat, cum)  0.92% of Total
         .          .   4087:func parkunlock_c(gp *g, lock unsafe.Pointer) bool {
         .      480ms   4088:	unlock((*mutex)(lock))
         .          .   4089:	return true
         .          .   4090:}
         .          .   4091:
         .          .   4092:// park continuation on g0.
         .          .   4093:func park_m(gp *g) {
ROUTINE ======================== runtime.pthread_mutex_unlock in /opt/homebrew/Cellar/go/1.24.3/libexec/src/runtime/sys_darwin.go
     150ms      150ms (flat, cum)  0.29% of Total
         .          .    527:func pthread_mutex_unlock(m *pthreadmutex) int32 {
     150ms      150ms    528:	ret := libcCall(unsafe.Pointer(abi.FuncPCABI0(pthread_mutex_unlock_trampoline)), unsafe.Pointer(&m))
         .          .    529:	KeepAlive(m)
         .          .    530:	return ret
         .          .    531:}
         .          .    532:func pthread_mutex_unlock_trampoline()
         .          .    533:
ROUTINE ======================== runtime.unlock in /opt/homebrew/Cellar/go/1.24.3/libexec/src/runtime/lock_spinbit.go
         0      2.65s (flat, cum)  5.06% of Total
         .          .    252:func unlock(l *mutex) {
         .      2.65s    253:	unlockWithRank(l)
         .          .    254:}
         .          .    255:
         .          .    256:// We might not be holding a p in this code.
         .          .    257://
         .          .    258://go:nowritebarrier
ROUTINE ======================== runtime.unlock2 in /opt/homebrew/Cellar/go/1.24.3/libexec/src/runtime/lock_spinbit.go
     120ms      2.69s (flat, cum)  5.14% of Total
      10ms       10ms    259:func unlock2(l *mutex) {
         .          .    260:	gp := getg()
         .          .    261:
      50ms       50ms    262:	prev8 := atomic.Xchg8(key8(&l.key), 0)
         .          .    263:	if prev8&mutexLocked == 0 {
         .          .    264:		throw("unlock of unlocked lock")
         .          .    265:	}
         .          .    266:
      60ms       60ms    267:	if prev8&mutexSleeping != 0 {
         .      2.39s    268:		unlock2Wake(l)
         .          .    269:	}
         .          .    270:
         .      180ms    271:	gp.m.mLockProfile.recordUnlock(l)
         .          .    272:	gp.m.locks--
         .          .    273:	if gp.m.locks < 0 {
         .          .    274:		throw("runtimeÂ·unlock: lock count")
         .          .    275:	}
         .          .    276:	if gp.m.locks == 0 && gp.preempt { // restore the preemption request in case we've cleared it in newstack
ROUTINE ======================== runtime.unlock2Wake in /opt/homebrew/Cellar/go/1.24.3/libexec/src/runtime/lock_spinbit.go
      20ms      2.39s (flat, cum)  4.56% of Total
         .          .    284:func unlock2Wake(l *mutex) {
         .          .    285:	v := atomic.Loaduintptr(&l.key)
         .          .    286:
         .          .    287:	// On occasion, seek out and wake the M at the bottom of the stack so it
         .          .    288:	// doesn't starve.
         .          .    289:	antiStarve := cheaprandn(mutexTailWakePeriod) == 0
         .          .    290:	if !(antiStarve || // avoiding starvation may require a wake
      10ms       10ms    291:		v&mutexSpinning == 0 || // no spinners means we must wake
      10ms       10ms    292:		mutexPreferLowLatency(l)) { // prefer waiters be awake as much as possible
         .          .    293:		return
         .          .    294:	}
         .          .    295:
         .          .    296:	for {
         .          .    297:		if v&^mutexMMask == 0 || v&mutexStackLocked != 0 {
         .          .    298:			// No waiting Ms means nothing to do.
         .          .    299:			//
         .          .    300:			// If the stack lock is unavailable, its owner would make the same
         .          .    301:			// wake decisions that we would, so there's nothing for us to do.
         .          .    302:			//
         .          .    303:			// Although: This thread may have a different call stack, which
         .          .    304:			// would result in a different entry in the mutex contention profile
         .          .    305:			// (upon completion of go.dev/issue/66999). That could lead to weird
         .          .    306:			// results if a slow critical section ends but another thread
         .          .    307:			// quickly takes the lock, finishes its own critical section,
         .          .    308:			// releases the lock, and then grabs the stack lock. That quick
         .          .    309:			// thread would then take credit (blame) for the delay that this
         .          .    310:			// slow thread caused. The alternative is to have more expensive
         .          .    311:			// atomic operations (a CAS) on the critical path of unlock2.
         .          .    312:			return
         .          .    313:		}
         .          .    314:		// Other M's are waiting for the lock.
         .          .    315:		// Obtain the stack lock, and pop off an M.
         .          .    316:		next := v | mutexStackLocked
         .          .    317:		if atomic.Casuintptr(&l.key, v, next) {
         .          .    318:			break
         .          .    319:		}
         .          .    320:		v = atomic.Loaduintptr(&l.key)
         .          .    321:	}
         .          .    322:
         .          .    323:	// We own the mutexStackLocked flag. New Ms may push themselves onto the
         .          .    324:	// stack concurrently, but we're now the only thread that can remove or
         .          .    325:	// modify the Ms that are sleeping in the list.
         .          .    326:
         .          .    327:	var committed *m // If we choose an M within the stack, we've made a promise to wake it
         .          .    328:	for {
         .          .    329:		headM := v &^ mutexMMask
         .          .    330:		flags := v & (mutexMMask &^ mutexStackLocked) // preserve low bits, but release stack lock
         .          .    331:
         .       10ms    332:		mp := mutexWaitListHead(v).ptr()
         .          .    333:		wakem := committed
         .          .    334:		if committed == nil {
         .          .    335:			if v&mutexSpinning == 0 || mutexPreferLowLatency(l) {
         .          .    336:				wakem = mp
         .          .    337:			}
         .          .    338:			if antiStarve {
         .          .    339:				// Wake the M at the bottom of the stack of waiters. (This is
         .          .    340:				// O(N) with the number of waiters.)
         .          .    341:				wakem = mp
         .          .    342:				prev := mp
         .          .    343:				for {
         .          .    344:					next := wakem.mWaitList.next.ptr()
         .          .    345:					if next == nil {
         .          .    346:						break
         .          .    347:					}
         .          .    348:					prev, wakem = wakem, next
         .          .    349:				}
         .          .    350:				if wakem != mp {
         .          .    351:					prev.mWaitList.next = wakem.mWaitList.next
         .          .    352:					committed = wakem
         .          .    353:				}
         .          .    354:			}
         .          .    355:		}
         .          .    356:
         .          .    357:		if wakem == mp {
         .          .    358:			headM = uintptr(mp.mWaitList.next) &^ mutexMMask
         .          .    359:		}
         .          .    360:
         .          .    361:		next := headM | flags
         .          .    362:		if atomic.Casuintptr(&l.key, v, next) {
         .          .    363:			if wakem != nil {
         .          .    364:				// Claimed an M. Wake it.
         .      2.36s    365:				semawakeup(wakem)
         .          .    366:			}
         .          .    367:			break
         .          .    368:		}
         .          .    369:
         .          .    370:		v = atomic.Loaduintptr(&l.key)
ROUTINE ======================== runtime.unlockWithRank in /opt/homebrew/Cellar/go/1.24.3/libexec/src/runtime/lockrank_off.go
         0      2.65s (flat, cum)  5.06% of Total
         .          .     34:func unlockWithRank(l *mutex) {
         .      2.65s     35:	unlock2(l)
         .          .     36:}
         .          .     37:
         .          .     38:// This function may be called in nosplit context and thus must be nosplit.
         .          .     39://
         .          .     40://go:nosplit
