Total: 52.53s
ROUTINE ======================== runtime.unlock2Wake in /opt/homebrew/Cellar/go/1.24.3/libexec/src/runtime/lock_spinbit.go
      40ms      2.24s (flat, cum)  4.26% of Total
         .          .    284:func unlock2Wake(l *mutex) {
         .          .    285:	v := atomic.Loaduintptr(&l.key)
         .          .    286:
         .          .    287:	// On occasion, seek out and wake the M at the bottom of the stack so it
         .          .    288:	// doesn't starve.
         .          .    289:	antiStarve := cheaprandn(mutexTailWakePeriod) == 0
         .          .    290:	if !(antiStarve || // avoiding starvation may require a wake
         .          .    291:		v&mutexSpinning == 0 || // no spinners means we must wake
      10ms       10ms    292:		mutexPreferLowLatency(l)) { // prefer waiters be awake as much as possible
         .          .    293:		return
         .          .    294:	}
         .          .    295:
         .          .    296:	for {
      20ms       20ms    297:		if v&^mutexMMask == 0 || v&mutexStackLocked != 0 {
         .          .    298:			// No waiting Ms means nothing to do.
         .          .    299:			//
         .          .    300:			// If the stack lock is unavailable, its owner would make the same
         .          .    301:			// wake decisions that we would, so there's nothing for us to do.
         .          .    302:			//
         .          .    303:			// Although: This thread may have a different call stack, which
         .          .    304:			// would result in a different entry in the mutex contention profile
         .          .    305:			// (upon completion of go.dev/issue/66999). That could lead to weird
         .          .    306:			// results if a slow critical section ends but another thread
         .          .    307:			// quickly takes the lock, finishes its own critical section,
         .          .    308:			// releases the lock, and then grabs the stack lock. That quick
         .          .    309:			// thread would then take credit (blame) for the delay that this
         .          .    310:			// slow thread caused. The alternative is to have more expensive
         .          .    311:			// atomic operations (a CAS) on the critical path of unlock2.
         .          .    312:			return
         .          .    313:		}
         .          .    314:		// Other M's are waiting for the lock.
         .          .    315:		// Obtain the stack lock, and pop off an M.
         .          .    316:		next := v | mutexStackLocked
      10ms       10ms    317:		if atomic.Casuintptr(&l.key, v, next) {
         .          .    318:			break
         .          .    319:		}
         .          .    320:		v = atomic.Loaduintptr(&l.key)
         .          .    321:	}
         .          .    322:
         .          .    323:	// We own the mutexStackLocked flag. New Ms may push themselves onto the
         .          .    324:	// stack concurrently, but we're now the only thread that can remove or
         .          .    325:	// modify the Ms that are sleeping in the list.
         .          .    326:
         .          .    327:	var committed *m // If we choose an M within the stack, we've made a promise to wake it
         .          .    328:	for {
         .          .    329:		headM := v &^ mutexMMask
         .          .    330:		flags := v & (mutexMMask &^ mutexStackLocked) // preserve low bits, but release stack lock
         .          .    331:
         .          .    332:		mp := mutexWaitListHead(v).ptr()
         .          .    333:		wakem := committed
         .          .    334:		if committed == nil {
         .          .    335:			if v&mutexSpinning == 0 || mutexPreferLowLatency(l) {
         .          .    336:				wakem = mp
         .          .    337:			}
         .          .    338:			if antiStarve {
         .          .    339:				// Wake the M at the bottom of the stack of waiters. (This is
         .          .    340:				// O(N) with the number of waiters.)
         .          .    341:				wakem = mp
         .          .    342:				prev := mp
         .          .    343:				for {
         .          .    344:					next := wakem.mWaitList.next.ptr()
         .          .    345:					if next == nil {
         .          .    346:						break
         .          .    347:					}
         .          .    348:					prev, wakem = wakem, next
         .          .    349:				}
         .          .    350:				if wakem != mp {
         .          .    351:					prev.mWaitList.next = wakem.mWaitList.next
         .          .    352:					committed = wakem
         .          .    353:				}
         .          .    354:			}
         .          .    355:		}
         .          .    356:
         .          .    357:		if wakem == mp {
         .          .    358:			headM = uintptr(mp.mWaitList.next) &^ mutexMMask
         .          .    359:		}
         .          .    360:
         .          .    361:		next := headM | flags
         .          .    362:		if atomic.Casuintptr(&l.key, v, next) {
         .          .    363:			if wakem != nil {
         .          .    364:				// Claimed an M. Wake it.
         .      2.20s    365:				semawakeup(wakem)
         .          .    366:			}
         .          .    367:			break
         .          .    368:		}
         .          .    369:
         .          .    370:		v = atomic.Loaduintptr(&l.key)
